ğŸ› ï¸ InstalaÃ§Ã£o
1ï¸âƒ£ Verifique se o Ollama estÃ¡ instalado
Abra o terminal e digite:

ollama --version
Se nÃ£o estiver instalado, siga o passo 2.

2ï¸âƒ£ Instale o Ollama
ğŸ”µ Windows
Baixe e instale diretamente do site:

ğŸ‘‰ https://ollama.com/download

ğŸŸ¢ Linux
Execute no terminal:

curl -fsSL https://ollama.com/install.sh | sh
Depois, inicie o serviÃ§o Ollama:

ollama serve

ğŸ macOS
Com Homebrew:

brew install ollama
Ou baixe o instalador em:

ğŸ‘‰ https://ollama.com/download

3ï¸âƒ£ Clone o Cerberus

git clone https://github.com/seu-user/cerberus-llm
cd cerberus-llm
bash install_cerberus.sh

4ï¸âƒ£ Crie o modelo personalizado
Com o Ollama rodando, crie o modelo Cerberus com base no Mistral:

ollama create cerberus -f Modelfile

5ï¸âƒ£ Rode o Cerberus

ollama run cerberus
ğŸ’¬ Exemplo de uso
Digite uma pergunta:

Como a memÃ³ria cache L1 influencia o desempenho de um sistema embarcado?
E receba uma resposta tÃ©cnica e direta!

ğŸ“ Estrutura do Projeto

cerberus-llm/
â”œâ”€â”€ Modelfile         # ConfiguraÃ§Ã£o do modelo personalizado
â”œâ”€â”€ exemplos/         # Exemplos de perguntas e respostas
â”œâ”€â”€ install_cerberus.sh
â””â”€â”€ README.md
ğŸ”— Base e Tecnologias
ğŸ” Modelo: Mistral 7B

âš™ï¸ ExecuÃ§Ã£o local via Ollama

ğŸ§  PersonalizaÃ§Ã£o via prompt engineering

ğŸ‘¨â€ğŸ’» Autor
Feito por Adriano Guedes Ferraz
